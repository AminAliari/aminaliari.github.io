<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?port=1313&amp;mindelay=10&amp;v=2" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<title>Optimizing a Motion Blur Effect - Amin&#39;s Den</title>

<meta name="description" content="Photo: The Last of Us Part II  Recently, I explored different Motion Blur techniques being used in the video game industry. Now, I have decided to jot down the result of this research.
At the first glance, I found several per-pixel motion blur approaches, but they did not impress me. To explain why I was not satisfied with the results, some history lessons are required!
Screen-space Motion Blur My main issue with the trivial approaches was that they would only take the camera motion into account.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/motionblur/" />

<link href="http://localhost:1313/css/stylesheet.min.c5f8f2549279b67d7472698057ccf873f52e6a24092d620f61495bd68940c025.css" integrity="sha256-xfjyVJJ5tn10cmmAV8z4c/UuaiQJLWIPYUlb1olAwCU=" rel="preload stylesheet"
    as="style">

<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<meta name="generator" content="Hugo 0.75.1" />

</head>

<body class="single dark" id="top">
<header class="header">
    <nav class="nav">
        <p class="logo"><a href="http://localhost:1313/">Amin&#39;s Den</a></p>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="http://localhost:1313/about/">
                    <span>
                        About
                    </span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/">
                    <span>
                        Archives
                    </span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/series/">
                    <span>
                        Series
                    </span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/">
                    <span>
                        Tags
                    </span>
                </a>
            </li>
        </ul>
    </nav>
</header>
    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      Optimizing a Motion Blur Effect
    </h1>
    <div class="post-meta">
      <time>October 25, 2020</time>&nbsp;·&nbsp;5 min
    </div>
  </header>
  <figure class="entry-cover">
    <img src="http://localhost:1313/posts/motionblur/images/final.png" alt="cover image for Optimizing a Motion Blur Effect">
  </figure>
  <div class="post-content"><h6 id="photo-the-last-of-us-part-ii">Photo: The Last of Us Part II</h6>
<hr>
<p>Recently, I explored different Motion Blur techniques being used in the video game industry. Now, I have decided to jot down the result of this research.</p>
<p>At the first glance, I found several per-pixel motion blur approaches, but they did not impress me. To explain why I was not satisfied with the results, some history lessons are required!</p>
<h1 id="screen-space-motion-blur">Screen-space Motion Blur</h1>
<p>My main issue with the trivial approaches was that they would only take the camera motion into account. The problem with this approach is that the movement of the object would not make a discernible motion trail. Basically, in motion blur effects based on the camera&rsquo;s position and orientation, one rendering pass would be dedicated to finding the difference of pixels' position between the current frame and the previous one. The result of this render pass is stored in a render target, commonly, called Screen velocity vectors.</p>
<h1 id="a-better-approach">A better approach</h1>
<p>As I was continuing my research, one method particularly caught my eye. It was called <em>A Reconstruction Filter for Plausible Motion Blur</em> by Prof. Morgan McGuire.</p>
<p>Let&rsquo;s first see how this method works:</p>
<ol>
<li>
<p>Render the scene</p>
</li>
<li>
<p>Store the depth buffer. This is needed at the filter stage to preserve the sharpness of the edges.</p>
</li>
</ol>
<figure>
    <img src="images/gbuffer.png"/> 
</figure>

<h1 id="heading"></h1>
<ol start="3">
<li>Create the velocity buffer based on current and the previous objects world matrices, and store it as half-vectors.</li>
</ol>
<figure>
    <img src="images/velocity-buffer.png"/> 
</figure>

<h1 id="heading-1"></h1>
<ol start="4">
<li>Divide the velocity buffer into KxK tiles which will generate a [Width/K]x[Height/K] buffer.</li>
</ol>
<h1 id="heading-2"></h1>
<ol start="5">
<li>Find the greatest velocity vector in each tile based on its length.</li>
</ol>
<figure>
    <img src="images/velocity-max.png"/> 
</figure>

<h1 id="heading-3"></h1>
<ol start="6">
<li>Find the greatest velocity vector between all neighbors of each tile.</li>
</ol>
<figure>
    <img src="images/tile-max.png"/> 
</figure>

<h1 id="heading-4"></h1>
<ol start="7">
<li>Apply the blur filters.</li>
</ol>
<pre><code>function reconstruct(X, C, V, Z, NeighborMax):
   // Largest velocity in the neighborhood
   Let ~vN = NeighborMax[bX/kc]   
   if ||~vN|| ≤ ε + 0.5px: return C[X] // No blur

   // Sample the current point
   weight = 1.0/||V[X]||; sum = C[X] · weight
	
   // Take S − 1 additional neighbor samples
   let j = random(−0.5, 0.5)
   for i ∈ [0, S), i 6= (S − 1)/2:
      // Choose evenly placed filter taps along ±~vN,
      // but jitter the whole filter to prevent ghosting
      t = mix (−1.0, 1.0,(i + j + 1.0)/(S + 1.0))
      let Y = bX + ~vN · t + 0.5pxc // round to nearest

      // Fore- vs. background classification of Y relative to X
      let f = softDepthCompare(Z[X], Z[Y])
      let b = softDepthCompare(Z[Y], Z[X])

      // Case 1: Blurry Y in front of any X
      αY = f · cone(Y, X, V[Y]) +

      // Case 2: Any Y behind blurry X;estimate background
      b · cone(X, Y, V[X]) +

      // Case 3: Simultaneously blurry X and Y
      cylinder(Y, X, V[Y]) · cylinder(X, Y, V[X]) · 2

      // Accumulate
      weight += αY ; sum += αY · C[Y]

   return sum/weight
</code></pre><h1 id="benefits">Benefits</h1>
<p>There are several benefits that this method brings to the table.</p>
<ul>
<li>Parallelism and cache efficiency</li>
<li>A line-gathering filter</li>
<li>Needs only to search along NeighborMax.</li>
<li>Compute NeighborMax with two m-way parallel-gather operations on n/k2</li>
<li>The entire algorithm is O(kn/m)-time</li>
<li>Exhibits both high memory locality and parallel memory access.</li>
<li>Keeps the edges sharp while blurring the moving objects</li>
<li>Early out when there is no movement going on in the radius of that pixel</li>
</ul>
<p>If you want to learn more about this method, you can read the paper down below in the references. Also, I need to mention that although there are many improvements that this method introduces, there exist more sophisticated and state-of-the-art solutions being used in the AAA games. There are many great GDC and SIGGRAPH talks on this topic, so I am sure you can find them yourself</p>
<h4 id="heading-5"></h4>
<h1 id="an-optimized-implementation">An optimized implementation</h1>
<p>Alright, enough with the introductions; now, it is time to talk about what I was able to do!</p>
<p>First of all, if this approach is considered to be a good one then what is the problem? Well, even though today&rsquo;s GPUs are way more competent than the ones back when this paper was first published (2012), it is always a good idea to find the optimum solution; especially when there is room for optimization.
So here is what I did to implement an optimized version of this method:</p>
<ol>
<li>Generally, memory bandwidth is scarce. Moreover, It gets even worse when we are talking about Graphics cards. Therefore, we do not want to store all of our buffers such as the velocity buffer and the ones generated from it into render targets. On the other hand, GPUs are more than okay to do a bunch of extra matrix multiplications on the fly. For this reason, it is feasible to upload all of our entities world matrices (both the current frame and the one before that) to the GPU which would basically be the same as our scene tree. This way we can calculate the objects' motion vector without excessive read or write of textures, and discard the result afterward.</li>
</ol>
<h1 id="heading-6"></h1>
<ol start="2">
<li>This whole approach is quite suitable for compute shaders. Yet, you may ask what advantages it has over our normal fragment/pixel shaders. Well, an architectural advantage of compute shaders for image processing is that they skip the ROP step.</li>
</ol>
<h1 id="heading-7"></h1>
<ol start="3">
<li>To further optimize this method, we have to pay heed to our memory access pattern. In the <em>Neighbor pass</em> where we find the most dominant velocity vector between the neighbors, we are essentially accessing 8 neighbors, but these tiles share a lot of neighbors. As a result, should we cache the result of our texture fetch in shared memories for each local thread group, we can avoid the extra costly texture fetches in the future.</li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Alright, finally, it is time to show some numbers. I implemented this project using The Forge rendering framework, and then ran it on an Nvidia&rsquo;s GTX 970. The results were as follow:</p>
<figure>
    <img src="images/result.png"/> 
</figure>

<h1 id="disclaimer">Disclaimer</h1>
<p>I wrote this article solely for educational purposes; therefore, I would not be surprised if I made a bunch of severe errors. In that case, I will be more than honored to be enlightened. For that, please use one of the contact methods provided on the Homepage. Thank you!</p>
<h4 id="heading-8"></h4>
<h1 id="references">References</h1>
<ol>
<li>McGuire M., Hennessy P., Bukowski M., Osman B.: <a href="https://casual-effects.com/research/McGuire2012Blur/McGuire12Blur.pdf" target="_blank">A reconstruction filter for plausible motion blur</a>
. In <em>Proc. ACM i3D</em> (2012), pp. 135– 42. 2, 8</li>
<li>Chapman J.: <a href="http://john-chapman-graphics.blogspot.com/2013/01/per-object-motion-blur.html" target="_blank">Per-Object Motion Blur</a>
</li>
<li>The Forge - <a href="https://github.com/ConfettiFX/The-Forge" target="_blank">Github</a>
</li>
</ol>
</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/computer-graphics">computer graphics</a></li>
      <li><a href="http://localhost:1313/tags/rendering">rendering</a></li>
      <li><a href="http://localhost:1313/tags/motion-blur">motion blur</a></li>
      <li><a href="http://localhost:1313/tags/optimization">optimization</a></li>
      <li><a href="http://localhost:1313/tags/velocity-buffer">velocity buffer</a></li>
    </ul>
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="http://localhost:1313/">Amin&#39;s Den</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo️️</a>️</span>
    <span>&middot;</span>
    <span>Theme️ <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top">
    <button class="top-link" id="top-link" type="button">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
            <path d="M12 6H0l6-6z" /></svg>
    </button>
</a>
<script src="http://localhost:1313/js/highlight.min.min.e7afc2928c0925d65c4732dfebe147014d91299a98e819e4b42f25c4fa68e91c.js" integrity="sha256-56/CkowJJdZcRzLf6&#43;FHAU2RKZqY6BnktC8lxPpo6Rw="></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();

            document.querySelector(this.getAttribute("href")).scrollIntoView({
                behavior: "smooth"
            });
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }
</script>
</body>

</html>